{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Assignment\" data-toc-modified-id=\"Final-Project-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Final Project Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Options-of-Project-Scenario\" data-toc-modified-id=\"Options-of-Project-Scenario-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Options of Project Scenario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scenario-1\" data-toc-modified-id=\"Scenario-1-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Scenario 1</a></span></li><li><span><a href=\"#Scenario-2\" data-toc-modified-id=\"Scenario-2-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Scenario 2</a></span></li><li><span><a href=\"#Scenario-3\" data-toc-modified-id=\"Scenario-3-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Scenario 3</a></span></li><li><span><a href=\"#Scenario-4\" data-toc-modified-id=\"Scenario-4-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Scenario 4</a></span></li></ul></li><li><span><a href=\"#Data-Set-Consideration\" data-toc-modified-id=\"Data-Set-Consideration-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Set Consideration</a></span></li></ul></li><li><span><a href=\"#Your-Final-Report\" data-toc-modified-id=\"Your-Final-Report-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Your Final Report</a></span></li><li><span><a href=\"#Team-1-Project:-Data-Wrangling-of-Streaming-Platforms\" data-toc-modified-id=\"Team-1-Project:-Data-Wrangling-of-Streaming-Platforms-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Team 1 Project: Data Wrangling of Streaming Platforms</a></span></li><li><span><a href=\"#Problem-Statement:\" data-toc-modified-id=\"Problem-Statement:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Problem Statement:</a></span><ul class=\"toc-item\"><li><span><a href=\"#User-Persona:\" data-toc-modified-id=\"User-Persona:-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>User Persona:</a></span></li><li><span><a href=\"#Problem-Statement:\" data-toc-modified-id=\"Problem-Statement:-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Problem Statement:</a></span></li><li><span><a href=\"#Background-on-Streaming-Platforms\" data-toc-modified-id=\"Background-on-Streaming-Platforms-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Background on Streaming Platforms</a></span></li></ul></li><li><span><a href=\"#Data-Acquisition:\" data-toc-modified-id=\"Data-Acquisition:-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Acquisition:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Streaming-platform-dataset-(structured-data)\" data-toc-modified-id=\"Streaming-platform-dataset-(structured-data)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Streaming platform dataset (structured data)</a></span></li><li><span><a href=\"#Show-ratings-from-IMDb-(structured-data)\" data-toc-modified-id=\"Show-ratings-from-IMDb-(structured-data)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Show ratings from IMDb (structured data)</a></span></li><li><span><a href=\"#Show-ratings-from-Metacritic-(unstructured-data)\" data-toc-modified-id=\"Show-ratings-from-Metacritic-(unstructured-data)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Show ratings from Metacritic (unstructured data)</a></span></li></ul></li><li><span><a href=\"#Shared-functions:\" data-toc-modified-id=\"Shared-functions:-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Shared functions:</a></span><ul class=\"toc-item\"><li><span><a href=\"#movecol---Shift-columns-in-dataframe\" data-toc-modified-id=\"movecol---Shift-columns-in-dataframe-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span><code>movecol</code> - Shift columns in dataframe</a></span></li><li><span><a href=\"#get_lowercase_columns---Convert-columns-cells-to-lower-cases\" data-toc-modified-id=\"get_lowercase_columns---Convert-columns-cells-to-lower-cases-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span><code>get_lowercase_columns</code> - Convert columns cells to lower cases</a></span></li><li><span><a href=\"#...-add-more-shared-functions-here-...\" data-toc-modified-id=\"...-add-more-shared-functions-here-...-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>... add more shared functions here ...</a></span></li></ul></li><li><span><a href=\"#Data-Cleaning,-Enrichment-and-Aggregation\" data-toc-modified-id=\"Data-Cleaning,-Enrichment-and-Aggregation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Cleaning, Enrichment and Aggregation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-and-combine-platform-files-into-dataframes\" data-toc-modified-id=\"Read-and-combine-platform-files-into-dataframes-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Read and combine platform files into dataframes</a></span></li><li><span><a href=\"#Clean-'rating'-and-'duration'-columns\" data-toc-modified-id=\"Clean-'rating'-and-'duration'-columns-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Clean <em>'rating'</em> and <em>'duration'</em> columns</a></span><ul class=\"toc-item\"><li><span><a href=\"#Copy-misplaced-duration-values-from-'ratings'--to-'duration'-column\" data-toc-modified-id=\"Copy-misplaced-duration-values-from-'ratings'--to-'duration'-column-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Copy misplaced duration values from <em>'ratings'</em>  to <em>'duration'</em> column</a></span></li><li><span><a href=\"#Split-'duration'-data-into-numerical-and-units\" data-toc-modified-id=\"Split-'duration'-data-into-numerical-and-units-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Split <em>'duration'</em> data into numerical and units</a></span></li><li><span><a href=\"#Create-a-new-column-to-aggregate-'rating'-data\" data-toc-modified-id=\"Create-a-new-column-to-aggregate-'rating'-data-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>Create a new column to aggregate <em>'rating'</em> data</a></span></li></ul></li><li><span><a href=\"#Genre-Cleanup\" data-toc-modified-id=\"Genre-Cleanup-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Genre Cleanup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Group-genres,-and-create-'one-hot'-encoding-of-major-genres\" data-toc-modified-id=\"Group-genres,-and-create-'one-hot'-encoding-of-major-genres-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>Group genres, and create 'one-hot' encoding of major genres</a></span></li></ul></li><li><span><a href=\"#Process-'director'-column\" data-toc-modified-id=\"Process-'director'-column-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Process <em>'director'</em> column</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distribution-of-number-of-directors-in-the-dataset\" data-toc-modified-id=\"Distribution-of-number-of-directors-in-the-dataset-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Distribution of number of directors in the dataset</a></span></li><li><span><a href=\"#Summarize-directors---create-a-new-'main_director'-column\" data-toc-modified-id=\"Summarize-directors---create-a-new-'main_director'-column-7.4.2\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;</span>Summarize directors - create a new <em>'main_director'</em> column</a></span></li></ul></li><li><span><a href=\"#Process-'cast'-column\" data-toc-modified-id=\"Process-'cast'-column-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Process <em>'cast'</em> column</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summarize-cast---create-new-columns-to-capture-important-cast-data\" data-toc-modified-id=\"Summarize-cast---create-new-columns-to-capture-important-cast-data-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>Summarize cast - create new columns to capture important cast data</a></span></li></ul></li><li><span><a href=\"#Incorporate-IMDb-critic-ratings-to-the-titles\" data-toc-modified-id=\"Incorporate-IMDb-critic-ratings-to-the-titles-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Incorporate IMDb critic ratings to the titles</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-critic-ratings-to-the-existing-dataframe\" data-toc-modified-id=\"Merge-critic-ratings-to-the-existing-dataframe-7.6.1\"><span class=\"toc-item-num\">7.6.1&nbsp;&nbsp;</span>Merge critic ratings to the existing dataframe</a></span></li><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-7.6.2\"><span class=\"toc-item-num\">7.6.2&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Incorporate-Metacritic-ratings-to-the-titles\" data-toc-modified-id=\"Incorporate-Metacritic-ratings-to-the-titles-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Incorporate Metacritic ratings to the titles</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-critic-ratings-to-the-existing-dataframe\" data-toc-modified-id=\"Merge-critic-ratings-to-the-existing-dataframe-7.7.1\"><span class=\"toc-item-num\">7.7.1&nbsp;&nbsp;</span>Merge critic ratings to the existing dataframe</a></span></li><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-7.7.2\"><span class=\"toc-item-num\">7.7.2&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Map-numbers-in-columns-into-targeted-histogram-bin-range\" data-toc-modified-id=\"Map-numbers-in-columns-into-targeted-histogram-bin-range-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Map numbers in columns into targeted histogram bin range</a></span></li></ul></li><li><span><a href=\"#Stats-and-Visualization\" data-toc-modified-id=\"Stats-and-Visualization-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Stats and Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Statistics\" data-toc-modified-id=\"Simple-Statistics-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Simple Statistics</a></span></li><li><span><a href=\"#Null-analysis\" data-toc-modified-id=\"Null-analysis-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Null analysis</a></span></li><li><span><a href=\"#Distribution-of-types-of-shows-by-platform\" data-toc-modified-id=\"Distribution-of-types-of-shows-by-platform-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Distribution of types of shows by platform</a></span><ul class=\"toc-item\"><li><span><a href=\"#Number-of-shows-by-type-on-each-respective-platform\" data-toc-modified-id=\"Number-of-shows-by-type-on-each-respective-platform-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Number of shows by type on each respective platform</a></span></li><li><span><a href=\"#Proportion-of-shows-by-type-on-each-respective-platform\" data-toc-modified-id=\"Proportion-of-shows-by-type-on-each-respective-platform-8.3.2\"><span class=\"toc-item-num\">8.3.2&nbsp;&nbsp;</span>Proportion of shows by type on each respective platform</a></span></li></ul></li><li><span><a href=\"#IMDb-ratings-distribution\" data-toc-modified-id=\"IMDb-ratings-distribution-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>IMDb ratings distribution</a></span></li><li><span><a href=\"#Rating-distrubution-by-genre\" data-toc-modified-id=\"Rating-distrubution-by-genre-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Rating distrubution by genre</a></span></li><li><span><a href=\"#Analysis-of-time-gap-between-show-release-year-and-date-it-was-added-to-a-platform\" data-toc-modified-id=\"Analysis-of-time-gap-between-show-release-year-and-date-it-was-added-to-a-platform-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>Analysis of time gap between show release year and date it was added to a platform</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.184269Z",
     "start_time": "2022-04-19T12:19:01.421145Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.189447Z",
     "start_time": "2022-04-19T12:19:02.186065Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# additional libraries and settings\n",
    "\n",
    "#rcParams are the parameters that can be set in the .matplotlibrc file\n",
    "from matplotlib import rcParams\n",
    "\n",
    "#An instance of RcParams for handling default matplotlib values with x, y\n",
    "rcParams['figure.figsize'] = 5, 5\n",
    "\n",
    "#Import missingno\n",
    "import missingno as missno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.262228Z",
     "start_time": "2022-04-19T12:19:02.191193Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Team1_Project/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1409/2053729126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set working directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Team1_Project/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Team1_Project/'"
     ]
    }
   ],
   "source": [
    "# set working directory\n",
    "os.chdir('Team1_Project/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Final Project Assignment\n",
    "\n",
    "The primary purpose to achieve is understanding of Data Wrangling Phase undertaken as part of the Data Preparation task.  The task undertaken and its result should exhibit participant's in-depth understanding in the phases of Data Wrangling, which requires participant to show a methodical step-by-step of achieving each of these phases.  For example, in the cleaning phase, first dissect into the various applicable steps and apply onto the dataset through any applicable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Options of Project Scenario\n",
    "\n",
    "### Scenario 1\n",
    "You can imagine the wrangling task is catered for data science Modelling phase. You are to prepare the modelling using dataset and apply the various phases and cover largely what have been taught. \n",
    "\n",
    "### Scenario 2\n",
    "You can imagine the wrangling task is designed for a System. There could be various sources(structured + unstructured) where you combine the files together using python (using identifier).\n",
    "\n",
    "### Scenario 3\n",
    "You can imagine you are wrangling from various sources i.e. twitter (need to apply API in advance). These will go to the reporting for instance business reporting, steering committee, general meeting, etcâ€¦\n",
    "\n",
    "### Scenario 4\n",
    "Any other permissible scenario that exhibits what have been largely taught and application of various phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Data Set Consideration\n",
    "\n",
    "Your dataset must at least satisfy option 1 below. Option 2 is optional and is highly encouraged.\n",
    "1. Small dataset (between 1k - 100k)\n",
    "2. (optional) Mid-size dataset (between 100k - 10mil)\n",
    "\n",
    "You need to apply what you have learnt to the following phases:\n",
    "* Data Discovery\n",
    "* Data Structuring\n",
    "* Data Cleaning and Validation\n",
    "* Data Enrichment\n",
    "* Data Aggregation\n",
    "\n",
    "You should take note of the following consideration:\n",
    "* Your dataset should include at least one less structured data\n",
    "* You should use more than 1 dataset for certain phases such as Data Enrichment/Blending i.e. (1) x 2 dataset + (2) x 2 dataset: >4 dataset\n",
    "* As this is a group work, you are expected to start early searching for the appropriate dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Final Report\n",
    "\n",
    "Write your group report here together with the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team 1 Project: Data Wrangling of Streaming Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "## User Persona:\n",
    "The average person who is considering which streaming platform to subscribe to, and want to make a more informed decision based on the content that each platform provides.\n",
    "\n",
    "## Problem Statement:\n",
    "\n",
    "Provide data to help users decide which platform(s) to subscribe to. This can be based on IMDB ratings, genre, number of movies/TV shows offered, the speed at which released titles are added to the platform, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T12:38:42.624153Z",
     "start_time": "2022-04-10T12:38:42.615100Z"
    }
   },
   "source": [
    "## Background on Streaming Platforms\n",
    "- Netflix:\n",
    "    - Netflix has over 8000 movies or tv shows available on their platform, as of mid-2021, they have over 200M Subscribers globally.\n",
    " \n",
    "- Amazon Prime:\n",
    "    - Amazon Prime has close to 10,000 movies or tv shows available on their platform, as of mid-2021, they have over 200M Subscribers globally.\n",
    "    \n",
    "- Disney+:\n",
    "    - Disney+ has close to 1,300 movies or tv shows available on their platform, as of mid-2021, they have over 116M Subscribers globally.\n",
    "    \n",
    "- Hulu:\n",
    "    - Hulu is an online movie and tv shows streaming platform owned by The Walt Disney Company. Hulu is exclusive to the United States and is not available in other countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition:\n",
    "## Streaming platform dataset (structured data)\n",
    "- Four structured data files from Kaggle, one for each streaming platform (Netflix, Disney+, Amazon Prime, Hulu)\n",
    "- Include details on shows that are being aired on these platforms, including title, maturity ratings, duration, year of production, release dates, show description, etc.\n",
    "- Combined, this dataset provides c. 22,000 titles across four platforms for analysis.\n",
    "\n",
    "*Data Source:*\n",
    "- https://www.kaggle.com/shivamb/netflix-shows?select=netflix_titles.csv\n",
    "- https://www.kaggle.com/shivamb/amazon-prime-movies-and-tv-shows\n",
    "- https://www.kaggle.com/shivamb/disney-movies-and-tv-shows\n",
    "- https://www.kaggle.com/shivamb/hulu-movies-and-tv-shows\n",
    "\n",
    "## Show ratings from IMDb (structured data)\n",
    "- Dataset from Kaggle includes IMDb and Rotten Tomatoes ratings of shows hosted on the four streaming platforms above.\n",
    "\n",
    "*Data Source:*\n",
    "- https://www.kaggle.com/code/ruchi798/movies-data-collection-eda-using-tableau/data\n",
    "\n",
    "## Show ratings from Metacritic (unstructured data)\n",
    "- Dataset scraped from Metacritic website\n",
    "\n",
    "*Data Source:*\n",
    "- https://www.metacritic.com/browse/movies/score/metascore/all/filtered?view=condensed&page=\n",
    "- https://www.metacritic.com/browse/tv/score/metascore/all/filtered?view=condensed&sort=desc&page="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Shared functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `movecol` - Shift columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.330984Z",
     "start_time": "2022-04-19T12:19:02.264350Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def movecol(df, cols_to_move=[], ref_col='', place='After'):\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    if place == 'After':\n",
    "        seg1 = cols[:list(cols).index(ref_col) + 1]\n",
    "        seg2 = cols_to_move\n",
    "    if place == 'Before':\n",
    "        seg1 = cols[:list(cols).index(ref_col)]\n",
    "        seg2 = cols_to_move + [ref_col]\n",
    "    \n",
    "    seg1 = [i for i in seg1 if i not in seg2]\n",
    "    seg3 = [i for i in cols if i not in seg1 + seg2]\n",
    "    \n",
    "    return(df[seg1 + seg2 + seg3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.612395Z",
     "start_time": "2022-04-19T12:19:02.332568Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "scrolled": true,
    "tags": [
     "vincent_needs_clean_categorical_csv_as_test_input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_c = pd.read_csv(\"./Combined_Platforms.csv\")\n",
    "df_c.dropna(inplace=True)\n",
    "# display(df_c.head())\n",
    "categorical_col = [\"platform\"]\n",
    "# display(df_c[categorical_col].head())\n",
    "bool_col = [\"type\"]\n",
    "# categorical_col = list(set(categorical_col) - set(bool_col))\n",
    "categorical_dict, value_counts_dict, max_num_rows = get_category_count_dict(df=df_c,categorical_col=categorical_col)\n",
    "\n",
    "cate_col = list(categorical_dict.keys())\n",
    "ordi_col = list(map(lambda x: f\"{x}_ordi\", cate_col))\n",
    "\n",
    "def get_cate_to_ordi_col_wise(x_name, x, dummy_series, categorical_dict=categorical_dict):\n",
    "    return categorical_dict[x].index(x_name)\n",
    "\n",
    "df_c[ordi_col] = df_c[cate_col].apply(lambda x: x.apply(get_cate_to_ordi_col_wise, args=(x.name, x)))\n",
    "display(df_c[cate_col+ordi_col])\n",
    "# del df\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.620614Z",
     "start_time": "2022-04-19T12:17:44.832Z"
    },
    "code_folding": [
     8
    ],
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./Combined_Platforms.csv\")\n",
    "print(df.shape)\n",
    "#drop missing date_added\n",
    "print(f\"missing values: date_added {df.date_added.isna().sum()}, release_year {df.release_year.isna().sum()} \")\n",
    "df = df[df.date_added.isna()==False] \n",
    "print(df.shape)\n",
    "\n",
    "gap_year, gap_day = get_gap_between_dates(df, \t\n",
    "    column_date_start=\"release_year\", format_start = \"%Y-%m-%d\", \n",
    "    column_date_end=\"date_added\", format_end=\"%Y\",\n",
    "    gap_type=\"Year\")\n",
    "\n",
    "display(df[[\"release_year\", \"date_added\", col_out, \"gap_day_\"]])\n",
    "# print(df[gap_year].value_counts())\n",
    "output_dir = \"10_temporary_output/\"\n",
    "df.to_csv(output_dir + \"date_gap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T02:20:09.857005Z",
     "start_time": "2022-04-14T02:20:09.847329Z"
    },
    "hide_input": true
   },
   "source": [
    "## `get_lowercase_columns` - Convert columns cells to lower cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.621674Z",
     "start_time": "2022-04-19T12:17:44.839Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "purpose: todo\n",
    "input: df, columns to be coverted\n",
    "output: name of converted col\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_lowercase_columns(df, columns: list, output_column_suffix=\"_\"):\n",
    "    output_column = []\n",
    "    for c in columns:\n",
    "        output_col_name = f\"{c}_{output_column_suffix}\"\n",
    "        output_column.append(output_col_name)\n",
    "        df[output_col_name] = df.title.apply(\n",
    "            lambda x: \" \".join(x.split()).lower())\n",
    "    return output_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.622841Z",
     "start_time": "2022-04-19T12:17:44.845Z"
    }
   },
   "outputs": [],
   "source": [
    "# example:\n",
    "\n",
    "df = pd.read_csv(\"netflix_titles.csv\")\n",
    "# display(df.head(2))\n",
    "input_col = [\"director\", \"cast\"]\n",
    "converted_col = get_lowercase_columns(df, columns=input_col, output_column_suffix=\"_lower\")\n",
    "display(df[input_col+converted_col].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-11T11:34:14.287Z"
    }
   },
   "source": [
    "## ... add more shared functions here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Enrichment and Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and combine platform files into dataframes\n",
    "- Use `comb_platform()` function to generate a dataframe that combines all the data from all platform files\n",
    "- This function takes as input a text file `platform_files.txt` comprising identifiers as well as the file names of all platform files\n",
    "- Returns the combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.624168Z",
     "start_time": "2022-04-19T12:17:44.857Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a dictionary comprising an identifier as well as the name of the data file\n",
    "def gen_platform_dict(platform_files):\n",
    "    f = open(platform_files, 'r+')\n",
    "    platform_dict = {}\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        p_file = [x.strip() for x in line.split(':')]\n",
    "        platform_dict[p_file[0]] = p_file[1]\n",
    "    \n",
    "    return platform_dict\n",
    "\n",
    "\n",
    "# iteratively create the dataframes, as well as specifying the datatypes in the dataframes\n",
    "# most of the datatypes are specified as strings, except for 'date_added' and 'release_year' which are dates\n",
    "def comb_platform(platform_files):\n",
    "    \n",
    "    platform_dict = gen_platform_dict(platform_files)\n",
    "    \n",
    "    df_comb = pd.DataFrame()\n",
    "    \n",
    "    for name, file in platform_dict.items():\n",
    "        df_tmp = pd.read_csv(file, \n",
    "                             dtype={\n",
    "                                 'show_id': str,\n",
    "                                 'type': str,\n",
    "                                 'title': str,\n",
    "                                 'director': str,\n",
    "                                 'cast': str,\n",
    "                                 'country': str,\n",
    "                                 'rating': str,\n",
    "                                 'duration': str,\n",
    "                                 'listed_in': str,\n",
    "                                 'description': str\n",
    "                             },\n",
    "                             parse_dates=['date_added', 'release_year'])\n",
    "        \n",
    "        # rename the 'show_id' column (which is redundant) to 'platform'\n",
    "        # and populate the 'platform' column with respective platform names\n",
    "        df_tmp = df_tmp.rename(columns={'show_id':'platform'})\n",
    "        df_tmp['platform'] = name\n",
    "        \n",
    "        df_comb = pd.concat([df_comb, df_tmp], ignore_index=True)\n",
    "    \n",
    "    df_comb['release_year'] = df_comb['release_year'].dt.year\n",
    "    return df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.625306Z",
     "start_time": "2022-04-19T12:17:44.862Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "df = comb_platform('platform_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.626327Z",
     "start_time": "2022-04-19T12:17:44.868Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clean *'rating'* and *'duration'* columns\n",
    "- There are a number of values in the **\"rating\"** (maturity rating) column indicating the duration of the title e.g. (2 Seasons, 79 Min). This should logically should be in the 'duration' column\n",
    "- Further investigation shows that these values are indeed misplaced and should be in the the **'duration'** column, where the values are Null\n",
    "- See results below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The 'rating' column has data on duration of the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.627384Z",
     "start_time": "2022-04-19T12:17:44.875Z"
    }
   },
   "outputs": [],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     - The time duration data on the 'rating' column corresponds to null values in the 'duration' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.628384Z",
     "start_time": "2022-04-19T12:17:44.883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df.loc[df['duration'].isnull() & ~(df['rating'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.629430Z",
     "start_time": "2022-04-19T12:17:44.887Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Copy misplaced duration values from *'ratings'*  to *'duration'* column\n",
    "- The function `correct_ratings_to_duration` copies misplaced duration values from **'ratings'** to **'duration'** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.630364Z",
     "start_time": "2022-04-19T12:17:44.891Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_ratings_to_duration(df):\n",
    "    \n",
    "    # swap misplaced duration values from 'ratings' to 'duration' column\n",
    "    df.loc[(df['duration'].isnull() & df['rating'].astype(str).str.contains('min|Season')), ['rating', 'duration']] = \\\n",
    "       df.loc[(df['duration'].isnull() & df['rating'].astype(str).str.contains('min|Season')), ['duration', 'rating']].values\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split *'duration'* data into numerical and units\n",
    "- **'duration'** has both the numerical data as well as the unit (seasons, minutes)\n",
    "- We will split this into two columns (create a new column **'durating_type'** to record the duration type)\n",
    "- The function `split_duration` splits 'duration' into numerical data as well as units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.631382Z",
     "start_time": "2022-04-19T12:17:44.897Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_duration(df):\n",
    "    \n",
    "    # create a new column 'duration_type' to record the unit of the duration\n",
    "    # and leave only the quantity in the 'duration' column\n",
    "    df[['duration','duration_type']] = df.duration.str.split(expand=True)\n",
    "    \n",
    "    # in 'duration_type' column, unify 'Seasons' and 'Season' as the same unit of measure\n",
    "    df.duration_type.replace({'Seasons':'Season'},inplace=True) \n",
    "    \n",
    "    # move 'duration_type' column next to 'duration'\n",
    "    df = movecol(df, cols_to_move=['duration_type'], ref_col='duration', place='After')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.632354Z",
     "start_time": "2022-04-19T12:17:44.903Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = correct_ratings_to_duration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.633344Z",
     "start_time": "2022-04-19T12:17:44.909Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = split_duration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.634347Z",
     "start_time": "2022-04-19T12:17:44.917Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.635449Z",
     "start_time": "2022-04-19T12:17:44.922Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing, can be removed\n",
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.636463Z",
     "start_time": "2022-04-19T12:17:44.925Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing and verification, can be removed\n",
    "df.duration.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.637485Z",
     "start_time": "2022-04-19T12:17:44.928Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing and verification, can be removed\n",
    "df.duration_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T17:48:24.332669Z",
     "start_time": "2022-04-10T17:48:24.323229Z"
    },
    "collapsed": true
   },
   "source": [
    "### Create a new column to aggregate *'rating'* data\n",
    "- Although the maturity ratings are now clean, there are too many of them (and different platform seem to have different ways of classifying them)\n",
    "- To aid analysis, we will create a new column called **targeted_age** to sort them into similar age buckets:\n",
    "\n",
    "|targeted_age|rating|\n",
    "|---|:--|\n",
    "|Kids|7+, ALL, ALL_AGES, G, TV-G, TV-Y|\n",
    "|Older Kids|16+, PG, TV-PG, TV-Y7, TV-Y7-FV|\n",
    "|Teens|13+, 16, AGES_16_, PG-13, TV-14|\n",
    "|Adults|18+, AGES_18_, NC-17, R, TV-MA|\n",
    "|Unrated|NOT RATED, NOT_RATE, NR, TV-NR, UNRATED, UR|\n",
    "\n",
    "- The function `create_rating_age_bucket` creates the new column above and buckets the maturity ratings into the ones defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.638596Z",
     "start_time": "2022-04-19T12:17:44.938Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function returns a dictionary of age ratings\n",
    "def age_bucket_dict(age_bucket_file):\n",
    "    f = open(age_bucket_file, 'r+')\n",
    "    rating_age_dict = {}\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        r = [x.strip() for x in line.split(',')]\n",
    "        rating_age_dict[r[0]] = r[1]\n",
    "    \n",
    "    return rating_age_dict\n",
    "\n",
    "# create a new column 'targeted_age' in the dataframe\n",
    "# and bucket the show ratings according to age brackets (defined in the age_bucket_file)\n",
    "def create_rating_age_bucket(df, age_bucket_file):\n",
    "    \n",
    "    rating_age_dict = age_bucket_dict(age_bucket_file)\n",
    "    \n",
    "    df['targeted_age'] = df['rating'].replace(rating_age_dict)\n",
    "    \n",
    "    df = movecol(df, cols_to_move=['targeted_age'], ref_col='rating', place='After')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.639639Z",
     "start_time": "2022-04-19T12:17:44.942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_bucket_file = 'rating_age_bucket.csv'\n",
    "df = create_rating_age_bucket(df, age_bucket_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.641548Z",
     "start_time": "2022-04-19T12:17:44.949Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Cleanup\n",
    "- The data for the genre of a show is in the **'listed_in'** column\n",
    "- As reflected below, a show can have multiple, and the definition of genres are not uniform across platforms\n",
    "- Hence, we clean up the genre with a two-step process:\n",
    "    1. List all the individual genres in the dataframe, and combine them into major genres (see example of grouping below) : \n",
    "    [insert image]\n",
    "    2. Create a 'one-hot' encoding of these major genres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - A show can have many genres (in the 'listed_in' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.642503Z",
     "start_time": "2022-04-19T12:17:44.956Z"
    }
   },
   "outputs": [],
   "source": [
    "df['listed_in'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group genres, and create 'one-hot' encoding of major genres\n",
    "- The genre grouping details is passed to the function as a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.643498Z",
     "start_time": "2022-04-19T12:17:44.968Z"
    },
    "code_folding": [
     18,
     44,
     51,
     58
    ],
    "collapsed": true,
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    check if row's detailed genres in \"listed_in\" appear in major genres\n",
    "input:     \n",
    "    4_genre_cleaning.txt : major genres grouping\n",
    "    dataframe with col \"listed_in\"\n",
    "output: \n",
    "    dataframe with columns of major genres in \"4_genre_cleaning.txt\"\n",
    "history:\n",
    "    20220408_0617: add flag __DEBUG_GENRE__ to hide debugging print\n",
    "               add verification code \"genre_dict_col_sum\" (vefified using Combined_Platforms_20220408_0627.csv)\n",
    "\n",
    "\"\"\"\n",
    "#------------\n",
    "#copy API: start\n",
    "#------------\n",
    "__DEBUG_GENRE__ = 0\n",
    "\n",
    "\n",
    "def get_genre_dict(genre_filepath):\n",
    "    f = open(genre_filepath, \"r+\")\n",
    "    genre_dict = {}\n",
    "    for line in f.readlines():\n",
    "        # print(f\"line \\n{line}\")\n",
    "        is_detailed_genre = (line[0] == \"\\t\")\n",
    "        line = \" \".join(line.strip().lower().split())\n",
    "        # print(f\"line.strip {is_detailed_genre} \\n{line}\")\n",
    "        if is_detailed_genre:\n",
    "            genre_dict[current_genre].append(line)\n",
    "            # print(f\"appended {line} to genre group {current_genre} \")\n",
    "        else:\n",
    "            current_genre = line\n",
    "            genre_dict[current_genre] = []\n",
    "            # print(f\"created genre group {current_genre}\")\n",
    "\n",
    "    def print_dict(in_dict):\n",
    "        for t, k in enumerate(in_dict.keys()):\n",
    "            print(t + 1, k)\n",
    "            print(f\"\\t {in_dict[k]}\")\n",
    "\n",
    "    if __DEBUG_GENRE__:\n",
    "        print_dict(genre_dict)\n",
    "\n",
    "    return genre_dict\n",
    "\n",
    "\n",
    "def is_in_major_genre(comma_separated_list: list, major_genre: str,\n",
    "                      genre_dict: dict):\n",
    "    listed_strip = [\n",
    "        \" \".join(c.split()) for c in comma_separated_list.split(\",\")\n",
    "    ]\n",
    "    # print(\"is_in_major_genre\", listed_strip,  set(listed_strip))\n",
    "    inner = set(listed_strip) & set(genre_dict[major_genre])\n",
    "    # print(\"\\t\\t\", {set(genre_dict[major_genre])}, {list(inner)}, {len(list(inner))})\n",
    "    return len(inner) > 0\n",
    "\n",
    "\n",
    "genre_dict_col_sum = {\n",
    "    'action & adventure': 3845,\n",
    "    'anime': 1756,\n",
    "    'children & family': 3433,\n",
    "    'classics': 179,\n",
    "    'comedy': 5919,\n",
    "    'crime & mystery': 846,\n",
    "    'documentary': 3285,\n",
    "    'drama': 8174,\n",
    "    'horror': 1611,\n",
    "    'music': 703,\n",
    "    'others': 4158,\n",
    "    'romance': 1867,\n",
    "    'sci-fi & fantasy': 1232,\n",
    "    'sports': 561,\n",
    "    'thrillers': 2374,\n",
    "    'tv shows': 1976,\n",
    "    'international movies': 2752\n",
    "}\n",
    "\n",
    "\n",
    "# 'international movies': 2754}\n",
    "def is_df_series_exist_in_dict_values(\n",
    "    df: pd.DataFrame,\n",
    "    col_header: str,\n",
    "    genre_dict: dict,\n",
    "    prefix_1hot=\"_\",\n",
    "    genre_dict_col_sum: dict = genre_dict_col_sum):\n",
    "    lowercase_1space_header = \"dummy_header\"\n",
    "    df[lowercase_1space_header] = df[col_header].apply(\n",
    "        lambda x: \" \".join(x.split()).lower())\n",
    "    # display(df[[\"listed_in\",\"listed_in_lowercase_1space\"]])\n",
    "    # print(df[\"listed_in_lowercase_1space\"][:2], type(df[\"listed_in_lowercase_1space\"][0]))\n",
    "    col_1hot_list = []\n",
    "    for t, genre in enumerate(genre_dict.keys()):\n",
    "        col_1hot = f\"{prefix_1hot}{genre.title()}\"\n",
    "        col_1hot_list.append(col_1hot)\n",
    "        df[col_1hot] = df[lowercase_1space_header].apply(lambda x: int(\n",
    "            is_in_major_genre(comma_separated_list=x,\n",
    "                              major_genre=genre,\n",
    "                              genre_dict=genre_dict)))\n",
    "\n",
    "        if __DEBUG_GENRE__:\n",
    "            # print(t, genre, df[col_1hot].sum())\n",
    "            assert genre_dict_col_sum[genre] == df[col_1hot].sum(), print(\n",
    "                f\"column sum of '{genre}' is {df[col_1hot].sum()}, expected {genre_dict_col_sum[genre]}\"\n",
    "            )\n",
    "\n",
    "    # print(genre_dict_col_sum)\n",
    "\n",
    "    if __DEBUG_GENRE__:\n",
    "        df[\"len_listed_in\"] = df[lowercase_1space_header].apply(\n",
    "            lambda x: len(x.split(\",\")))\n",
    "        df[\"sum_genre\"] = df[col_1hot_list].agg(\"sum\", axis=1)\n",
    "        df[\"diff\"] = (df[\"len_listed_in\"] - df[\"sum_genre\"])\n",
    "        print(f\"df['diff] {sum(df['diff'])}\")\n",
    "\n",
    "    df.drop(columns=lowercase_1space_header, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#------------\n",
    "#copy API: end\n",
    "#------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.644459Z",
     "start_time": "2022-04-19T12:17:44.980Z"
    }
   },
   "outputs": [],
   "source": [
    "genre_dict = get_genre_dict(\"genre_cleaning.txt\")\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.645502Z",
     "start_time": "2022-04-19T12:17:44.988Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1hot = is_df_series_exist_in_dict_values(df=df, col_header=\"listed_in\", genre_dict=genre_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.646452Z",
     "start_time": "2022-04-19T12:17:45.002Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_1hot.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.647464Z",
     "start_time": "2022-04-19T12:17:45.011Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1hot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.648469Z",
     "start_time": "2022-04-19T12:17:45.021Z"
    }
   },
   "outputs": [],
   "source": [
    "df = is_df_series_exist_in_dict_values(df=df, col_header=\"listed_in\", genre_dict=genre_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.649406Z",
     "start_time": "2022-04-19T12:17:45.031Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process *'director'* column\n",
    "- Overall, the 'director' data is more than 50% complete, and is missing completely in the Hulu dataset. Despite this, We can still derive insights from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.650392Z",
     "start_time": "2022-04-19T12:17:45.047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dir = comb_platform('platform_files.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Comment:\n",
    "    We see that director and cast data are mainly missing from the Hulu dataset, but we can still have meaningful data from the other platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.651372Z",
     "start_time": "2022-04-19T12:17:45.061Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(2,2)\n",
    "\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "missno.matrix(df_dir[df_dir['platform']=='Amazon Prime'], ax=axarr[0][0], fontsize=10)\n",
    "axarr[0][0].set_title(\"Amazon Prime\")\n",
    "\n",
    "missno.matrix(df_dir[df_dir['platform']=='Disney+'], ax=axarr[0][1], fontsize=10)\n",
    "axarr[0][1].set_title(\"Disney+\")\n",
    "\n",
    "missno.matrix(df_dir[df_dir['platform']=='Hulu'], ax=axarr[1][0], fontsize=10)\n",
    "axarr[1][0].set_title(\"Hulu\")\n",
    "\n",
    "missno.matrix(df_dir[df_dir['platform']=='Netflix'], ax=axarr[1][1], fontsize=10)\n",
    "axarr[1][1].set_title(\"Netflix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of number of directors in the dataset\n",
    "\n",
    "- There may be more than one director for a show (separated by commas in the 'director' columns\n",
    "- We want to see how the number of directors are distributed within the dataset\n",
    "- We find most shows overwhelmingly have only one director (13,000+ shows), followed by two directors (1000+) which is an order of magnitude lower in quantity than movies with one director\n",
    "- We assume here that in the 'director' column, the directors are ordered by importance. Hence, we create a new column 'main_director' to capture that information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.652289Z",
     "start_time": "2022-04-19T12:17:45.077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dummy column to count the number of directors in each show\n",
    "df['director'] = df['director'].str.split(',')\n",
    "df['n_director'] = df['director'].str.len()\n",
    "\n",
    "# the distribution of number of directors:\n",
    "data = df['n_director'].value_counts()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize directors - create a new *'main_director'* column\n",
    "- The function `create_col_major_director` creates a new column outlining the main director of the show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.653291Z",
     "start_time": "2022-04-19T12:17:45.090Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# def create_col_main_director(df):\n",
    "#     # Step 1, we replace all NaN's in the director column with a blank string\n",
    "#     df['director']=df['director'].fillna(\"\")\n",
    "\n",
    "#     # Step 2, we create a new column to house only the main director\n",
    "#     df['main_director'] = df['director'].apply(lambda x : x.split(',')[0])\n",
    "    \n",
    "#     # Step 3, we re-arrange the layout of the columns\n",
    "#     df = movecol(df, cols_to_move=['main_director'], ref_col='director', place='After')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.654369Z",
     "start_time": "2022-04-19T12:17:45.102Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_col_main_director(df):\n",
    "\n",
    "    # Step 1, we create a new column to house only the main director\n",
    "    df['main_director'] = df['director'].apply(lambda x : x.split(',')[0] if not pd.isnull(x) else np.nan)\n",
    "    \n",
    "    # Step 2, we re-arrange the layout of the columns\n",
    "    df = movecol(df, cols_to_move=['main_director'], ref_col='director', place='After')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.655325Z",
     "start_time": "2022-04-19T12:17:45.117Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = comb_platform('platform_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.656268Z",
     "start_time": "2022-04-19T12:17:45.126Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = create_col_main_director(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.657229Z",
     "start_time": "2022-04-19T12:17:45.142Z"
    }
   },
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process *'cast'* column\n",
    "- Like 'director', the 'cast' data is more than 50% complete, and is missing completely in the Hulu dataset\n",
    "- As expected there is greater variability in the number of cast in show, although there are still a number of shows with only 1 cast (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.658233Z",
     "start_time": "2022-04-19T12:17:45.159Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = comb_platform('platform_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.659168Z",
     "start_time": "2022-04-19T12:17:45.173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dummy column to count the number of directors in each show\n",
    "df['n_cast'] = df['cast'].str.split(',')\n",
    "df['n_cast'] = df['n_cast'].str.len()\n",
    "\n",
    "# Print the distribution of number of cast\n",
    "data = df['n_cast'].value_counts()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize cast - create new columns to capture important cast data\n",
    "- The function `create_cols_main_cast` creates new columns summarizing the cast of the show\n",
    "- 'n_cast': outline the number of cast\n",
    "- 'main_cast_1': the first cast in the list of casts\n",
    "- 'main_cast_2': the second cast\n",
    "- etc and extract up to 6 casts, as that is the average number of casts in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.660190Z",
     "start_time": "2022-04-19T12:17:45.187Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cols_main_cast(df, n=2, set_n=False):\n",
    "    \n",
    "    # Step 1: create a column 'n_cast' to count the number of cast\n",
    "    # note this returns a null (NaN) value if the 'cast' column is a null (NaN)\n",
    "    df['n_cast'] = df['cast'].str.split(',')\n",
    "    df['n_cast'] = df['n_cast'].str.len()\n",
    "    \n",
    "    # step 2: determine the number of main cast columns to create:\n",
    "    #\n",
    "    # parameter 'set_n' lets user specify the number of cast columns to create:\n",
    "    #    if it is set to False, the function automatically create the x columns\n",
    "    #        where x is the average number of casts in the dataset\n",
    "    #    otherwise, \n",
    "    #        number of columns will be set to n, which is specified by the user\n",
    "    if not set_n:\n",
    "        n = int(df['n_cast'].mean()) \n",
    "    \n",
    "    # Step 3: Dynamically create new columns\n",
    "    for i in range(n):\n",
    "        col_name = 'main_cast_' + str(i+1)\n",
    "        df[col_name] = df['cast'].apply(lambda x : x.split(',')[i] if (not pd.isnull(x) and len(x.split(',')) >= (i+1))  else np.nan)\n",
    "  \n",
    "    \n",
    "    # Step 3, we re-arrange the layout of the columns (starting from 'n_cast' column)\n",
    "    col_list = list(df.columns)\n",
    "    start_col = col_list.index('n_cast')\n",
    "    cols_to_move = col_list[start_col:]\n",
    "    \n",
    "    df = movecol(df, cols_to_move, ref_col='cast', place='After')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.661115Z",
     "start_time": "2022-04-19T12:17:45.197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = comb_platform('platform_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.662121Z",
     "start_time": "2022-04-19T12:17:45.208Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = create_cols_main_cast(df, n=2, set_n=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.663151Z",
     "start_time": "2022-04-19T12:17:45.230Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate IMDb critic ratings to the titles\n",
    "- We supplement our combined dataset with critic ratings (IMDb) using a new dataset.\n",
    "- This dataset includes IMDb and Rotten Tomatoes ratings of shows hosted on the four streaming platforms above.\n",
    "\n",
    "*Data Source:*\n",
    "- https://www.kaggle.com/code/ruchi798/movies-data-collection-eda-using-tableau/data\n",
    "\n",
    "### Merge critic ratings to the existing dataframe\n",
    "- The function `merge_critic_ratings` incorporates the critic ratings (both IMDb and Rotten Tomatoes)\n",
    "- Merging is done based on commonality in the two dataframes across: (1) the show title (converted to all smallcaps and single spacing between words), (2) the release year. We include also the release year, as there are different shows with the same titles.\n",
    "\n",
    "### Comments\n",
    "- Based on the methodology above, the are only about 4,500 titles with IMDb ratings, which is only about 20% of all titles in our combined dataset. One reason could be that our ratings dataset only cover movies, and there may be inconsistencies between title names and release dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.664180Z",
     "start_time": "2022-04-19T12:17:45.243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function merges IMDb critic ratings to the existing dataset of platform shows\n",
    "# df_comb is the dataframe of combined platforms\n",
    "\n",
    "def merge_critic_ratings(rating_file, df_comb):\n",
    "    df_ratings = pd.read_csv(rating_file)\n",
    "    \n",
    "    # created a 'new_title' column (lower caps, only one space) for both dataframes\n",
    "    df_comb['new_title'] = df_comb['title'].apply(lambda x: \" \".join(x.split()).lower())\n",
    "    df_ratings['new_title'] = df_ratings['Title'].apply(lambda x: \" \".join(x.split()).lower())\n",
    "    \n",
    "    # incorporate the critic ratings using the merge function\n",
    "    df_new = pd.merge(df_comb, df_ratings[['new_title', 'Year','IMDb', 'Rotten Tomatoes', ]], how=\"left\", left_on=['new_title', 'release_year'], right_on=['new_title', 'Year'])\n",
    "    \n",
    "    # remove redundant columns\n",
    "    df_new = df_new.drop(['new_title', 'Year'], axis=1)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.665180Z",
     "start_time": "2022-04-19T12:17:45.255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "df_comb = comb_platform('platform_files.txt')\n",
    "rating_file = 'ratings_MoviesRatings.csv'\n",
    "\n",
    "df_comb = merge_critic_ratings(rating_file, df_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.666123Z",
     "start_time": "2022-04-19T12:17:45.269Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comb.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.667145Z",
     "start_time": "2022-04-19T12:17:45.281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imbd = df_comb[~df_comb['IMDb'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.668236Z",
     "start_time": "2022-04-19T12:17:45.314Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imbd.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Metacritic ratings to the titles\n",
    "- We supplement our combined dataset with Metacritic using the dataset scrapped from MetaCritic's website\n",
    "- This dataset includes both critic and user ratings\n",
    "\n",
    "### Merge critic ratings to the existing dataframe\n",
    "- The function `merge_metacritic_ratings` incorporates the Metacritic ratings\n",
    "- Merging is done based on commonality in the two dataframes across: (1) the show title (converted to all smallcaps and single spacing between words), (2) the release year. We include also the release year, as there are different shows with the same titles.\n",
    "\n",
    "### Comments\n",
    "- Based on the methodology above, the are only c. 3,000 titles with Metacritic ratings, which is only about 13% of all titles in our combined dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - First we process scrapped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.669184Z",
     "start_time": "2022-04-19T12:17:45.339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process scrapped dataset\n",
    "df_mt_movie = pd.read_csv(\"metacritic_movie_ratings.csv\")\n",
    "df_mt_tv = pd.read_csv(\"metacritic_tv_ratings.csv\")\n",
    "\n",
    "# include column to differentiate titles from Movies and TV Shows\n",
    "df_mt_movie['type'] = 'Movie'\n",
    "df_mt_tv['type'] = 'TV Show'\n",
    "\n",
    "# join these two datasets\n",
    "df_mt_rating = pd.concat([df_mt_movie, df_mt_tv], ignore_index=True)\n",
    "\n",
    "# drop titles with no release dates\n",
    "df_mt_rating.loc[df_mt_rating['Date_Released'].str.contains('TBA', case=False)] = np.nan\n",
    "df_mt_rating = df_mt_rating.dropna()\n",
    "\n",
    "# create a new column to record the release year of titles\n",
    "df_mt_rating['Date_Released'] = pd.to_datetime(df_mt_rating['Date_Released'])\n",
    "df_mt_rating['Year_Released'] = df_mt_rating['Date_Released'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Function below merges Metacritic critic ratings to the existing dataset of platform shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.670189Z",
     "start_time": "2022-04-19T12:17:45.355Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function merges Metacritic critic ratings to the existing dataset of platform shows\n",
    "# df_comb is the dataframe of combined platforms\n",
    "\n",
    "def merge_metacritic_ratings(df_mt_rating, df_comb):\n",
    "    \n",
    "    # created a 'new_title' column (lower caps, only one space) for both dataframes\n",
    "    df_comb['new_title'] = df_comb['title'].apply(lambda x: \" \".join(x.split()).lower())\n",
    "    df_mt_rating['new_title'] = df_mt_rating['Title'].apply(lambda x: \" \".join(x.split()).lower())\n",
    "    \n",
    "    # incorporate the critic ratings using the merge function\n",
    "    df_new = pd.merge(df_comb, df_mt_rating[['new_title', 'Year_Released','Metacritic_Score', 'Metacritic_User_Score', ]], how=\"left\", left_on=['new_title', 'release_year'], right_on=['new_title', 'Year_Released'])\n",
    "    \n",
    "    # remove redundant columns\n",
    "#     df_new = df_new.drop(['new_title', 'Year_Released'], axis=1)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.671455Z",
     "start_time": "2022-04-19T12:17:45.366Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comb = comb_platform('platform_files.txt')\n",
    "df_comb = merge_metacritic_ratings(df_mt_rating, df_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T08:36:23.206525Z",
     "start_time": "2022-04-19T08:36:23.200084Z"
    }
   },
   "source": [
    "    - Post merging, we see that we have c. 3,000 titles with Metacritic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.672624Z",
     "start_time": "2022-04-19T12:17:45.385Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comb.notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Map numbers in columns into targeted histogram bin range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.673864Z",
     "start_time": "2022-04-19T12:17:45.399Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find range between start and end date\n",
    "\"\"\"\n",
    "input : df, column_start_date, column_end_date, date_format = \"%Y-%m-%d\",\n",
    "output : column name of the gap\n",
    "added column: gap_{gap_type[0]}, gap_day\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_gap_between_dates(df,\n",
    "                          column_date_start: str,\n",
    "                          column_date_end: str,\n",
    "                          format_start=\"%Y-%m-%d\",\n",
    "                          format_end=\"%Y-%m-%d\",\n",
    "                          gap_type=\"Year\"):\n",
    "    gap_col = f\"gap_{gap_type}_\"\n",
    "    df[\"gap_day_\"] = pd.to_datetime(df[column_date_end], format=format_start) - \\\n",
    "        pd.to_datetime(df[column_date_start], format=format_end)\n",
    "    df[gap_col] = df[\"gap_day_\"].astype(f\"timedelta64[{gap_type[0].upper()}]\").astype(int)\n",
    "    return [gap_col, \"gap_day_\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "purpose: map single number into bin_range\n",
    "inputs:\n",
    "    input: single number to be mapped into bin_range\n",
    "outputs:\n",
    "    bin_range: [min(input_list), start_number_of_range1, start_number_of_range2, ..., max(input_list)]\n",
    "\n",
    "added column \n",
    "\"\"\"\n",
    "\n",
    "def binning(input, bin_range: list):\n",
    "    input = np.array([input])\n",
    "    bin_range = np.array(bin_range)\n",
    "    index = np.argmin(((input - bin_range) >= 0).astype(int)) - 1\n",
    "    bin = bin_range[index]\n",
    "    return bin\n",
    "\n",
    "# def binning_columns(df,\n",
    "#                     bin_range: list,\n",
    "#                     input_column=\"\",\n",
    "#                     output_column_suffix=\"_bin\",\n",
    "#                     is_plot=True):\n",
    "#     suffix = output_column_suffix\n",
    "#     output_col = f\"{input_column}_{suffix}\"\n",
    "#     df[output_col] = df[input_column].apply(lambda x: binning(x, bin_range))\n",
    "#     if is_plot:\n",
    "#         df[input_column].value_counts()[:20].sort_index().plot(kind=\"bar\")\n",
    "#         plt.show()\n",
    "#         df[output_col].value_counts().sort_index().plot(kind=\"bar\")\n",
    "#         plt.show()\n",
    "#     return output_col\n",
    "\n",
    "def binning_columns(\n",
    "    df, \n",
    "    bin_range: list,\n",
    "    input_column=\"\", \n",
    "    output_column_suffix=\"_bin\",\n",
    "    is_plot=True,\n",
    "    title=\"\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"Frequency\",\n",
    "):\n",
    "    assert(len(input_column)), print(\"Please assign name of the column to be binned to input_column.\")       \n",
    "        \n",
    "    if xlabel==\"\":\n",
    "        xlabel=input_column\n",
    "    \n",
    "    suffix = output_column_suffix\n",
    "    output_col = f\"{input_column}{suffix}\"\n",
    "    # assert df[input_column].min() < min(bin_range), print(f\"{input_column} min values ({df[input_column].min()}) is smaller than {bin_range}\")\n",
    "    # assert df[input_column].max() > max(bin_range), print(f\"{input_column} min values ({df[input_column].min()}) is bigger than {bin_range}\")\n",
    "    df[output_col] = df[input_column].apply(lambda x: binning(x, bin_range))\n",
    "    if is_plot:\n",
    "        df[input_column].value_counts()[:20].sort_index().plot(\n",
    "            kind=\"bar\", title=title, xlabel=xlabel, ylabel=ylabel\n",
    "            )\n",
    "        plt.show()\n",
    "        df[output_col].value_counts().sort_index().plot(\n",
    "            kind=\"bar\",  title = title, xlabel = f\"{xlabel}_binned\", ylabel = ylabel\n",
    "            )\n",
    "        plt.show()\n",
    "    return output_col\n",
    "# Find range between start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.675063Z",
     "start_time": "2022-04-19T12:17:45.414Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = comb_platform('platform_files.txt')\n",
    "#df = pd.read_csv(\"Combined_Platforms.csv\")\n",
    "df = df[df.date_added.isna() == False]  #remove rows with empty date_added\n",
    "\n",
    "gap_year, gap_day = get_gap_between_dates(df,\n",
    "                                          column_date_start=\"release_year\",\n",
    "                                          format_start=\"%Y-%m-%d\",\n",
    "                                          column_date_end=\"date_added\",\n",
    "                                          format_end=\"%Y\",\n",
    "                                          gap_type=\"Year\")\n",
    "\n",
    "df = df[df[gap_year] >= 0]\n",
    "bin_range = [df[gap_year].min(), 0, 1, 2, 5, 10, 20, df[gap_year].max() + 1]\n",
    "output_col = binning_columns(df=df, input_column=gap_year, bin_range=bin_range)\n",
    "display(df[[\"date_added\", \"release_year\", gap_year, output_col]].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.676218Z",
     "start_time": "2022-04-19T12:17:45.424Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T18:18:01.055338Z",
     "start_time": "2022-04-18T18:18:01.045573Z"
    }
   },
   "source": [
    "    - Generate clean and enriched dataframe using the cleanup functions from Section 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.677369Z",
     "start_time": "2022-04-19T12:17:45.440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate dataframe from cleanups described previously\n",
    "\n",
    "\n",
    "# combined platform files into a dataframe\n",
    "df = comb_platform('platform_files.txt')\n",
    "\n",
    "# clean up rating and duration columns\n",
    "df = correct_ratings_to_duration(df)\n",
    "df = split_duration(df)\n",
    "\n",
    "# create new column to aggregate rating data\n",
    "age_bucket_file = 'rating_age_bucket.csv'\n",
    "df = create_rating_age_bucket(df, age_bucket_file)\n",
    "\n",
    "# group genre and create one-hot encoding of major genres\n",
    "genre_dict = get_genre_dict(\"genre_cleaning.txt\")\n",
    "df = is_df_series_exist_in_dict_values(df=df, col_header=\"listed_in\", genre_dict=genre_dict)\n",
    "\n",
    "# summarize directors by creating a new 'main_director' column\n",
    "df = create_col_main_director(df)\n",
    "\n",
    "# summarize cast by creating a new 'main_cast' columns\n",
    "df = create_cols_main_cast(df, n=2, set_n=True)\n",
    "\n",
    "# merge critic ratings\n",
    "rating_file = 'ratings_MoviesRatings.csv'\n",
    "df = merge_critic_ratings(rating_file, df)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.678297Z",
     "start_time": "2022-04-19T12:17:45.466Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.679311Z",
     "start_time": "2022-04-19T12:17:45.481Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.680276Z",
     "start_time": "2022-04-19T12:17:45.499Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(2,2)\n",
    "\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "missno.matrix(df[df['platform']=='Amazon Prime'], ax=axarr[0][0], fontsize=10)\n",
    "axarr[0][0].set_title(\"Amazon Prime\")\n",
    "\n",
    "missno.matrix(df[df['platform']=='Disney+'], ax=axarr[0][1], fontsize=10)\n",
    "axarr[0][1].set_title(\"Disney+\")\n",
    "\n",
    "missno.matrix(df[df['platform']=='Hulu'], ax=axarr[1][0], fontsize=10)\n",
    "axarr[1][0].set_title(\"Hulu\")\n",
    "\n",
    "missno.matrix(df[df['platform']=='Netflix'], ax=axarr[1][1], fontsize=10)\n",
    "axarr[1][1].set_title(\"Netflix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.681303Z",
     "start_time": "2022-04-19T12:17:45.515Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Distribution of types of shows by platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.682251Z",
     "start_time": "2022-04-19T12:17:45.528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_show_type = df[['platform', 'type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of shows by type on each respective platform\n",
    "- Amazon Prime and Netflix has the most number of shows, and their titles focus more on movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.683268Z",
     "start_time": "2022-04-19T12:17:45.545Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_tab_shows = pd.crosstab(index=df_show_type['platform'], columns=df_show_type['type'])\n",
    "\n",
    "cross_tab_shows.plot(kind='bar', stacked=False, colormap='coolwarm', figsize=(10,6))\n",
    "\n",
    "plt.legend(loc='best', ncol=6)\n",
    "plt.xlabel('Platform')\n",
    "plt.ylabel('No. of shows on platform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of shows by type on each respective platform\n",
    "- In terms of proportions, Hulu has the highest proportion of TV-show offerings of all the platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.684242Z",
     "start_time": "2022-04-19T12:17:45.558Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_tab_show_type = pd.crosstab(index=df_show_type['platform'], columns=df_show_type['type'], normalize='index')\n",
    "\n",
    "cross_tab_show_type.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(10,6))\n",
    "\n",
    "plt.legend(loc='best', ncol=6)\n",
    "plt.xlabel('Platform')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb ratings distribution\n",
    "- The number of available IMDb ratings do not follow that of movies available on each platform\n",
    "- However, based on the data available, it seems that the overall ratings on each platform are similarly distributed, peaking somewhere between 6 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.685252Z",
     "start_time": "2022-04-19T12:17:45.572Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df[df['type']=='Movie'], x='platform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.686246Z",
     "start_time": "2022-04-19T12:17:45.578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imdb_plot = df[['platform', 'IMDb']]\n",
    "df_imdb_plot = df_imdb_plot.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.687273Z",
     "start_time": "2022-04-19T12:17:45.586Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df_imdb_plot, x='platform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.688249Z",
     "start_time": "2022-04-19T12:17:45.595Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_imdb_plot, row='platform', height=1.7, aspect=4,)\n",
    "g.map(sns.kdeplot, 'IMDb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating distrubution by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.689195Z",
     "start_time": "2022-04-19T12:17:47.352Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_list=[]\n",
    "for g in df[genre].unique():\n",
    "    df[df[]]\n",
    "    df_imdb_plot =  df[df[g]][['platform', 'IMDb']]\n",
    "    sns.FacetGrid(df_imdb_plot, row='platform', height=1.7, aspect=4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of time gap between show release year and date it was added to a platform\n",
    "- Based on the analysis, we see that Netflix has the highest proportion of new movies. On the data available, more than 50% of Netflix's shows are added on its platform less than two years after it was released. This is followed by Amazon Prime, in which c. 40% of shows were added on its platform after it was released.\n",
    "- Disney+ has distinctively more older shows on its platform. On the data available, where c. 50% of its titles has more than 10 years' gap between when it was released and listed on its platform. This could be due to the fact that Disney is a much older company has owns many classic titles which it then releases on its streaming platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.690186Z",
     "start_time": "2022-04-19T12:17:47.363Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_date = comb_platform('platform_files.txt')\n",
    "df_date = df_date[df_date.date_added.isna() == False]  #remove rows with empty date_added\n",
    "\n",
    "gap_year, gap_day = get_gap_between_dates(df_date,\n",
    "                                          column_date_start=\"release_year\",\n",
    "                                          format_start=\"%Y-%m-%d\",\n",
    "                                          column_date_end=\"date_added\",\n",
    "                                          format_end=\"%Y\",\n",
    "                                          gap_type=\"Year\")\n",
    "\n",
    "df_date = df_date[df_date[gap_year] >= 0]\n",
    "bin_range = [df_date[gap_year].min(), 0, 1, 2, 5, 10, 20, df_date[gap_year].max() + 1]\n",
    "output_col = binning_columns(df=df_date, input_column=gap_year, bin_range=bin_range, is_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.691201Z",
     "start_time": "2022-04-19T12:17:47.373Z"
    }
   },
   "outputs": [],
   "source": [
    "df_date.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.692139Z",
     "start_time": "2022-04-19T12:17:47.383Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_plt_gap = df_date[['platform', 'gap_Year___bin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.693087Z",
     "start_time": "2022-04-19T12:17:47.392Z"
    }
   },
   "outputs": [],
   "source": [
    "df_plt_gap.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.694063Z",
     "start_time": "2022-04-19T12:17:47.402Z"
    }
   },
   "outputs": [],
   "source": [
    "df_plt_gap['gap_Year___bin'] = df_plt_gap['gap_Year___bin'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.695013Z",
     "start_time": "2022-04-19T12:17:47.411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_tab_gap_prop = pd.crosstab(index=df_plt_gap['platform'], columns=df_plt_gap['gap_Year___bin'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.695964Z",
     "start_time": "2022-04-19T12:17:47.421Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_tab_gap = pd.crosstab(index=df_plt_gap['platform'], columns=df_plt_gap['gap_Year___bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.696894Z",
     "start_time": "2022-04-19T12:17:47.430Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_tab_gap_prop.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(10,6))\n",
    "\n",
    "plt.legend(loc='best', ncol=6)\n",
    "plt.xlabel('Platform')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.697902Z",
     "start_time": "2022-04-19T12:17:47.436Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_tab_gap_prop.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(10,6))\n",
    "\n",
    "plt.legend(loc='best', ncol=6)\n",
    "plt.xlabel('Platform')\n",
    "plt.ylabel('Percentage')\n",
    "\n",
    "for n, a in enumerate([*cross_tab_gap.index.values]):\n",
    "    gap = -0.05\n",
    "    for proportion in cross_tab_gap_prop.loc[a]:            \n",
    "        plt.text(x=n,\n",
    "                 y=proportion + gap,\n",
    "                 s=f'{np.round(proportion * 100, 1)}%', \n",
    "                 color=\"black\",\n",
    "                 fontsize=12,\n",
    "                 fontweight=\"bold\")\n",
    "        gap += proportion\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.698885Z",
     "start_time": "2022-04-19T12:17:47.441Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:19:02.699838Z",
     "start_time": "2022-04-19T12:17:47.444Z"
    }
   },
   "outputs": [],
   "source": [
    "df.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "505px",
    "width": "246px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
